{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhmoon/venvFL/env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../multiFL/sensIT/weights/combined_weights.pickle', 'rb') as f:\n",
    "    data1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "modals = ['acoustic', 'seismic', 'combined']\n",
    "for i in modals:\n",
    "    with open('/home/jhmoon/venvFL/2023-paper-Federated_Learning/multiFL/sensIT/weights/'+i+'_weights.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        parameters.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.341786446"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.16793582 + 1.73850626e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08748538,  0.11064506,  0.2118959 ,  0.00230611, -0.14275293,\n",
       "        0.08237765,  0.14774989,  0.10490526, -0.18172738,  0.24118485,\n",
       "       -0.08349203,  0.01528702, -0.11052497, -0.055976  ,  0.01051702,\n",
       "        0.08748504], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06365526,  0.12841164,  0.14366466, -0.02249604, -0.12527254,\n",
       "        0.01222751,  0.15078573,  0.13521458, -0.1635928 ,  0.1657714 ,\n",
       "       -0.12832288,  0.02810126, -0.09335352, -0.114896  ,  0.00706776,\n",
       "        0.03649428], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20220259,  0.13033639,  0.12800704, -0.06475664, -0.04006581,\n",
       "        0.12088747,  0.04621652,  0.18229635,  0.20121178,  0.16857632,\n",
       "        0.08287422, -0.11505114,  0.05854994,  0.04866898, -0.03849654,\n",
       "        0.014607  ], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[2][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01702065,  0.12313103,  0.1611892 , -0.02831552, -0.1026971 ,\n",
       "        0.07183088,  0.11491738,  0.14080541, -0.04803613,  0.19184418,\n",
       "       -0.04298024, -0.02388762, -0.04844285, -0.04073434, -0.00697059,\n",
       "        0.04619544], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(parameters[0][5] + parameters[1][5] + parameters[2][5]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01473692, -0.08233635, -0.04315492, ...,  0.02950101,\n",
       "         -0.04219197, -0.01716765],\n",
       "        [ 0.15205336,  0.0521142 ,  0.09288424, ...,  0.05788887,\n",
       "          0.10540593, -0.07192094],\n",
       "        [ 0.04559253, -0.06202829,  0.09280962, ...,  0.00753483,\n",
       "         -0.04888826, -0.04434061],\n",
       "        ...,\n",
       "        [-0.00599087, -0.12753661, -0.05109057, ...,  0.09543911,\n",
       "          0.05214577, -0.05690293],\n",
       "        [ 0.08031798,  0.06951158,  0.09535187, ..., -0.11202417,\n",
       "         -0.0997164 ,  0.07131494],\n",
       "        [-0.10287038, -0.15981415, -0.07832343, ...,  0.09636814,\n",
       "          0.07599012,  0.03378509]], dtype=float32),\n",
       " array([-0.02462531,  0.10708216,  0.02358895,  0.01630341, -0.00149157,\n",
       "         0.01118014,  0.09509651,  0.14081329, -0.06775125,  0.1346242 ,\n",
       "        -0.0275601 ,  0.08418267,  0.00502005, -0.05104493,  0.00807615,\n",
       "         0.04730692, -0.02745045,  0.04011538,  0.13539073,  0.03590843,\n",
       "         0.01804212,  0.15102242, -0.02633748,  0.01626055,  0.14902447,\n",
       "         0.01185911,  0.00730209,  0.01336316,  0.01026499, -0.01710096,\n",
       "         0.04891963,  0.16021146], dtype=float32),\n",
       " array([[ 0.1105883 , -0.02957789,  0.10713226, ..., -0.04029786,\n",
       "          0.05056925, -0.00323397],\n",
       "        [ 0.21134943, -0.14004648, -0.04106629, ...,  0.08490367,\n",
       "         -0.14617206,  0.05699175],\n",
       "        [-0.0273062 , -0.11790369, -0.13752054, ..., -0.06733797,\n",
       "         -0.06567348,  0.09550686],\n",
       "        ...,\n",
       "        [-0.10378648, -0.03169223,  0.03211205, ...,  0.03848834,\n",
       "          0.07517035, -0.01167099],\n",
       "        [ 0.1627591 , -0.1049365 , -0.11759349, ..., -0.06250164,\n",
       "         -0.00177806, -0.1121707 ],\n",
       "        [-0.11122965,  0.13205054, -0.16231807, ..., -0.08125199,\n",
       "         -0.0221804 , -0.07754634]], dtype=float32),\n",
       " array([-0.1367388 ,  0.0092509 , -0.1221626 ,  0.21686892, -0.01486774,\n",
       "         0.05822008, -0.05166466, -0.15053795,  0.10166902, -0.11060256,\n",
       "         0.04959071, -0.09531608,  0.11255109,  0.15294756,  0.18481916,\n",
       "         0.2131306 , -0.01538047, -0.14477888, -0.06585497, -0.16662383,\n",
       "         0.07081427, -0.12482625, -0.05841837,  0.20079532,  0.03648629,\n",
       "        -0.03083192,  0.2059428 , -0.03841928, -0.14513858,  0.15158439,\n",
       "         0.18290862,  0.02908142], dtype=float32),\n",
       " array([[-0.11053469, -0.02872389,  0.03956969, -0.16182964, -0.13694778,\n",
       "         -0.05663668,  0.07799813, -0.00995393,  0.0966761 ,  0.17039722,\n",
       "          0.03121798, -0.0830012 , -0.11931219,  0.01507   ,  0.08948804,\n",
       "         -0.03490842, -0.2358437 , -0.07473194, -0.11540306,  0.05112167,\n",
       "         -0.11953741,  0.01465151,  0.05375986,  0.0861061 , -0.02875707,\n",
       "          0.14595486, -0.05831525,  0.10225207,  0.01354336, -0.1488756 ,\n",
       "          0.07624493, -0.08834191],\n",
       "        [-0.05365017, -0.03108293, -0.15123512,  0.14539285,  0.14744364,\n",
       "          0.06776442, -0.07730064,  0.16119725, -0.0162688 , -0.10728945,\n",
       "         -0.12466384,  0.03213019, -0.06592572, -0.04607839, -0.1593924 ,\n",
       "         -0.06220949, -0.11476593, -0.09372849, -0.13173799, -0.10935433,\n",
       "          0.17698112, -0.05724141, -0.09246392,  0.17810367,  0.06549545,\n",
       "         -0.13661245, -0.16140696,  0.0170405 , -0.09938916,  0.11520118,\n",
       "         -0.16023754, -0.14900547],\n",
       "        [ 0.11990291, -0.07275169,  0.04071594, -0.08744963, -0.18102747,\n",
       "          0.05157531, -0.07317717,  0.13462825, -0.11797725, -0.01767259,\n",
       "         -0.07549103, -0.07629372, -0.01525976,  0.10681453,  0.00156994,\n",
       "         -0.03455888,  0.06332946, -0.04846247,  0.01057578, -0.12119728,\n",
       "          0.08776402,  0.20520875,  0.01687636,  0.19797546,  0.06482074,\n",
       "          0.18574548, -0.01554549, -0.00652568, -0.0282402 , -0.09461255,\n",
       "          0.11302338, -0.06120246],\n",
       "        [ 0.03835557,  0.16787454, -0.02516382,  0.20539823, -0.03241682,\n",
       "          0.1342024 ,  0.16270773, -0.09711339,  0.05682951,  0.14884074,\n",
       "         -0.13427323, -0.097995  , -0.12045673,  0.08428864,  0.06733856,\n",
       "         -0.01121994,  0.0928339 , -0.14292215,  0.09098662, -0.07979544,\n",
       "          0.1421609 , -0.12492817, -0.08808684,  0.10980204,  0.08823111,\n",
       "         -0.03803672, -0.14764845, -0.18508346,  0.12502928, -0.02747012,\n",
       "          0.02679362,  0.08506204],\n",
       "        [-0.00876069, -0.11120144, -0.07708565,  0.00259271, -0.04639353,\n",
       "         -0.01679708,  0.05509049, -0.16163115,  0.04405006, -0.00223063,\n",
       "         -0.14919867, -0.03108751,  0.00213348,  0.2154903 ,  0.2487743 ,\n",
       "          0.0089579 , -0.14471677,  0.12050582,  0.13810709,  0.10072791,\n",
       "         -0.14286296,  0.09138668,  0.03356427,  0.10866769, -0.12390625,\n",
       "          0.074614  ,  0.01266339,  0.17279345, -0.10496219,  0.17612672,\n",
       "          0.08861021,  0.07756709],\n",
       "        [-0.07988562,  0.05845056,  0.05523564, -0.08508484,  0.08126161,\n",
       "          0.15062678,  0.10954122,  0.15425318, -0.03184207, -0.16357015,\n",
       "         -0.03766098, -0.10516179,  0.15358181,  0.04299054, -0.15889695,\n",
       "         -0.14130047, -0.08993833,  0.09207986,  0.08440598,  0.01924029,\n",
       "          0.00413352,  0.12767744,  0.17613938, -0.15821947, -0.07580864,\n",
       "          0.0544139 , -0.13831526,  0.00689188,  0.06489229, -0.08843042,\n",
       "          0.12490167, -0.10432115],\n",
       "        [-0.08433946, -0.08308309, -0.07031625, -0.0147222 , -0.14668927,\n",
       "          0.06402315, -0.17720814, -0.06180987, -0.01307998,  0.0523363 ,\n",
       "          0.08677227, -0.01276978,  0.00756917, -0.078009  ,  0.01141231,\n",
       "          0.07768716, -0.19660921, -0.12600915,  0.0550033 , -0.15733877,\n",
       "          0.11834047, -0.02106749, -0.0804793 ,  0.14982088, -0.0696017 ,\n",
       "          0.09625272,  0.21455415,  0.1949733 ,  0.15644173, -0.12217501,\n",
       "          0.19779639,  0.10483418],\n",
       "        [ 0.0561515 ,  0.03313831, -0.06539188, -0.08487113,  0.07364851,\n",
       "         -0.07215758, -0.11215503,  0.14179862,  0.00627987, -0.148869  ,\n",
       "         -0.13459983,  0.03317844, -0.05017014, -0.06109267, -0.1145068 ,\n",
       "         -0.10407311,  0.17200208,  0.2158835 ,  0.12801494,  0.02345871,\n",
       "          0.07324784, -0.13108693, -0.01994601,  0.10334516, -0.06689479,\n",
       "          0.10474134, -0.125728  , -0.15158413,  0.09501231,  0.21111122,\n",
       "          0.18576945, -0.10946728],\n",
       "        [-0.15296984,  0.19453622, -0.11026466,  0.16723216,  0.24996896,\n",
       "          0.06742213,  0.19287965, -0.13346173,  0.21458532,  0.14721256,\n",
       "          0.21865508,  0.04427715,  0.17150488, -0.13041325, -0.01931365,\n",
       "          0.00086259,  0.09806384,  0.10813507,  0.09782023,  0.1543173 ,\n",
       "         -0.07998651,  0.03588198, -0.08808332,  0.03567829,  0.17404322,\n",
       "         -0.10368527, -0.10878678, -0.12398706,  0.19420955,  0.01718237,\n",
       "         -0.05957309, -0.02539266],\n",
       "        [ 0.02878404, -0.15817752, -0.04050197, -0.1438688 ,  0.10710704,\n",
       "          0.08847361,  0.08318219, -0.06112935, -0.15467471,  0.17715536,\n",
       "         -0.12815277, -0.13636312,  0.05051497,  0.18610893,  0.20551828,\n",
       "          0.10844866, -0.0760662 , -0.21427116, -0.05685639,  0.07568674,\n",
       "         -0.09677993, -0.00971655,  0.08117611, -0.00290452, -0.02772183,\n",
       "         -0.11982641,  0.03918922,  0.1023588 ,  0.09568747,  0.0381295 ,\n",
       "          0.11010048,  0.16321349],\n",
       "        [ 0.08544581,  0.1572408 ,  0.18554871,  0.09392652,  0.19358917,\n",
       "          0.16994159, -0.09887634, -0.10043439,  0.19317786,  0.1732206 ,\n",
       "         -0.03115288,  0.04678773,  0.20030037, -0.18704718, -0.11831823,\n",
       "          0.09340841,  0.04723592,  0.1017805 ,  0.08950695,  0.05544524,\n",
       "          0.21141414,  0.11086448, -0.10655907,  0.00743223,  0.09328942,\n",
       "          0.00930989, -0.11578184,  0.03024952, -0.02676842,  0.12243878,\n",
       "          0.03466642,  0.10285452],\n",
       "        [ 0.00911601, -0.08974838,  0.14144902,  0.07741109,  0.02348174,\n",
       "         -0.19845101,  0.04937107,  0.0097397 , -0.20091262, -0.16376927,\n",
       "          0.0016819 , -0.01725066,  0.09390531,  0.0487197 , -0.10863236,\n",
       "          0.0174978 ,  0.04495997,  0.020522  ,  0.12301042,  0.13227332,\n",
       "          0.04129473,  0.20780216, -0.12769635,  0.05383062, -0.07173314,\n",
       "         -0.00287018,  0.07290925,  0.16058846, -0.03311928,  0.04906375,\n",
       "          0.14782001,  0.04582688],\n",
       "        [ 0.00444747, -0.16560948, -0.09974597, -0.01027794, -0.00458173,\n",
       "         -0.04901631, -0.13160293, -0.11406931,  0.04248552,  0.02442867,\n",
       "         -0.17778327, -0.13723682, -0.10061625, -0.08738323,  0.19850916,\n",
       "          0.20993935, -0.0671628 ,  0.08242726,  0.03672417, -0.07171885,\n",
       "         -0.01432653, -0.00437667, -0.04500113,  0.15561701, -0.02747149,\n",
       "          0.13494404,  0.15899245,  0.16190802,  0.01737222,  0.03117506,\n",
       "          0.12588808, -0.05572734],\n",
       "        [ 0.02028885, -0.0249036 , -0.01254212, -0.07385495,  0.0296395 ,\n",
       "          0.01518648, -0.14672253, -0.05492066, -0.17235373,  0.10695704,\n",
       "          0.00101326,  0.04734293,  0.034019  ,  0.05467431,  0.18642922,\n",
       "          0.05032268,  0.02436095,  0.04059435,  0.11726961, -0.03916528,\n",
       "         -0.00214398, -0.10988699,  0.12626116,  0.06054669,  0.02139424,\n",
       "         -0.09347923,  0.05042395,  0.20046276, -0.07415626,  0.04903381,\n",
       "          0.12793346,  0.13312668],\n",
       "        [ 0.1838211 , -0.103384  , -0.10097304,  0.06575751, -0.03399892,\n",
       "         -0.12422189, -0.13036275,  0.13675772, -0.10947251,  0.17996341,\n",
       "         -0.10838813,  0.10766553, -0.04976275,  0.1938276 ,  0.24144831,\n",
       "          0.22398208, -0.08695821, -0.05432659, -0.06781153, -0.0300342 ,\n",
       "          0.07921096,  0.18460475, -0.05126848,  0.1666513 , -0.02559388,\n",
       "          0.06833907, -0.03793416,  0.102858  ,  0.0436468 ,  0.16717157,\n",
       "          0.18501662,  0.1896904 ],\n",
       "        [ 0.1300202 , -0.06069389,  0.11183929,  0.09321218,  0.06769831,\n",
       "          0.06028825, -0.06915695,  0.00351815, -0.13008797,  0.09108984,\n",
       "         -0.05293906, -0.14480385, -0.10913991, -0.06338146,  0.11654141,\n",
       "          0.20215185, -0.06862231, -0.10523935, -0.06008892, -0.14344813,\n",
       "          0.05803709,  0.16260892, -0.12719594,  0.1774823 ,  0.02387045,\n",
       "         -0.09592748,  0.18063682,  0.05796456, -0.08491234,  0.1050908 ,\n",
       "         -0.04395456,  0.07910751]], dtype=float32),\n",
       " array([ 0.20220259,  0.13033639,  0.12800704, -0.06475664, -0.04006581,\n",
       "         0.12088747,  0.04621652,  0.18229635,  0.20121178,  0.16857632,\n",
       "         0.08287422, -0.11505114,  0.05854994,  0.04866898, -0.03849654,\n",
       "         0.014607  ], dtype=float32),\n",
       " array([[ 0.07223269, -0.21015504,  0.0429032 , -0.27237752,  0.20026734,\n",
       "         -0.18477635,  0.14744002, -0.09534371, -0.21471211,  0.12361538,\n",
       "         -0.2518392 ,  0.16646324,  0.08433313,  0.09090932,  0.23069152,\n",
       "          0.23580766]], dtype=float32),\n",
       " array([0.19716904], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/acoustic.pickle', 'rb') as f:\n",
    "    data1 = pickle.load(f)\n",
    "with open('../Data/seismic.pickle', 'rb') as f:\n",
    "    data2 = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/combined.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.023321</td>\n",
       "      <td>-0.023527</td>\n",
       "      <td>-0.022061</td>\n",
       "      <td>-0.022273</td>\n",
       "      <td>-0.017079</td>\n",
       "      <td>0.018815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031178</td>\n",
       "      <td>-0.017721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038098</td>\n",
       "      <td>-0.035813</td>\n",
       "      <td>-0.036718</td>\n",
       "      <td>-0.038129</td>\n",
       "      <td>-0.033562</td>\n",
       "      <td>-0.038832</td>\n",
       "      <td>-0.038019</td>\n",
       "      <td>-0.037385</td>\n",
       "      <td>-0.034283</td>\n",
       "      <td>-0.039099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.136421</td>\n",
       "      <td>-0.092407</td>\n",
       "      <td>-0.131769</td>\n",
       "      <td>0.574799</td>\n",
       "      <td>-0.073130</td>\n",
       "      <td>0.021133</td>\n",
       "      <td>0.099456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.614752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042590</td>\n",
       "      <td>-0.045936</td>\n",
       "      <td>-0.045755</td>\n",
       "      <td>-0.045720</td>\n",
       "      <td>-0.045836</td>\n",
       "      <td>-0.045605</td>\n",
       "      <td>-0.044757</td>\n",
       "      <td>-0.046381</td>\n",
       "      <td>-0.043888</td>\n",
       "      <td>-0.045910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.134293</td>\n",
       "      <td>0.138123</td>\n",
       "      <td>0.525289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832403</td>\n",
       "      <td>0.659512</td>\n",
       "      <td>-0.047688</td>\n",
       "      <td>0.097587</td>\n",
       "      <td>0.014862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019597</td>\n",
       "      <td>-0.046355</td>\n",
       "      <td>-0.016725</td>\n",
       "      <td>-0.110025</td>\n",
       "      <td>-0.074977</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>-0.081088</td>\n",
       "      <td>-0.117135</td>\n",
       "      <td>0.124611</td>\n",
       "      <td>-0.084323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.712613</td>\n",
       "      <td>0.782345</td>\n",
       "      <td>0.190930</td>\n",
       "      <td>0.229841</td>\n",
       "      <td>0.560167</td>\n",
       "      <td>0.192150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783759</td>\n",
       "      <td>-0.025315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063093</td>\n",
       "      <td>-0.020576</td>\n",
       "      <td>-0.054475</td>\n",
       "      <td>0.056441</td>\n",
       "      <td>-0.033763</td>\n",
       "      <td>-0.020097</td>\n",
       "      <td>-0.038449</td>\n",
       "      <td>-0.030611</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>-0.029294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.197980</td>\n",
       "      <td>0.300174</td>\n",
       "      <td>0.130691</td>\n",
       "      <td>0.802940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>-0.009243</td>\n",
       "      <td>0.350689</td>\n",
       "      <td>0.281720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>-0.047797</td>\n",
       "      <td>-0.032164</td>\n",
       "      <td>-0.056727</td>\n",
       "      <td>-0.064532</td>\n",
       "      <td>-0.039657</td>\n",
       "      <td>-0.054612</td>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.046586</td>\n",
       "      <td>-0.053346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78818</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.012863</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>-0.018340</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.033045</td>\n",
       "      <td>0.098008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135341</td>\n",
       "      <td>0.145462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069438</td>\n",
       "      <td>-0.023398</td>\n",
       "      <td>-0.072769</td>\n",
       "      <td>-0.036016</td>\n",
       "      <td>-0.059330</td>\n",
       "      <td>-0.054264</td>\n",
       "      <td>0.068717</td>\n",
       "      <td>-0.082765</td>\n",
       "      <td>-0.063361</td>\n",
       "      <td>-0.058121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78819</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.024249</td>\n",
       "      <td>-0.024640</td>\n",
       "      <td>-0.021435</td>\n",
       "      <td>-0.038170</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>-0.011039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070916</td>\n",
       "      <td>0.121450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035993</td>\n",
       "      <td>-0.035162</td>\n",
       "      <td>-0.035777</td>\n",
       "      <td>-0.035664</td>\n",
       "      <td>-0.036003</td>\n",
       "      <td>-0.035998</td>\n",
       "      <td>-0.035592</td>\n",
       "      <td>-0.035893</td>\n",
       "      <td>-0.035913</td>\n",
       "      <td>-0.035753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78820</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.059299</td>\n",
       "      <td>0.027528</td>\n",
       "      <td>-0.092111</td>\n",
       "      <td>0.653803</td>\n",
       "      <td>0.296419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.040911</td>\n",
       "      <td>0.927669</td>\n",
       "      <td>-0.082818</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184531</td>\n",
       "      <td>-0.013405</td>\n",
       "      <td>0.358282</td>\n",
       "      <td>0.185890</td>\n",
       "      <td>0.470470</td>\n",
       "      <td>0.242505</td>\n",
       "      <td>0.240674</td>\n",
       "      <td>-0.109372</td>\n",
       "      <td>0.071049</td>\n",
       "      <td>0.615857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78821</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.017755</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.071065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>-0.013738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027309</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>-0.051606</td>\n",
       "      <td>-0.022456</td>\n",
       "      <td>0.132588</td>\n",
       "      <td>-0.101000</td>\n",
       "      <td>-0.098870</td>\n",
       "      <td>0.060842</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.055325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78822</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.018382</td>\n",
       "      <td>0.088634</td>\n",
       "      <td>0.170688</td>\n",
       "      <td>0.165125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045117</td>\n",
       "      <td>0.030288</td>\n",
       "      <td>0.053111</td>\n",
       "      <td>0.020284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068714</td>\n",
       "      <td>0.112066</td>\n",
       "      <td>0.031503</td>\n",
       "      <td>-0.077510</td>\n",
       "      <td>-0.098591</td>\n",
       "      <td>-0.038509</td>\n",
       "      <td>0.203295</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.095316</td>\n",
       "      <td>-0.097685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78823 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class         0         1         2         3         4         5  \\\n",
       "0          3 -0.023321 -0.023527 -0.022061 -0.022273 -0.017079  0.018815   \n",
       "1          1 -0.136421 -0.092407 -0.131769  0.574799 -0.073130  0.021133   \n",
       "2          3  0.134293  0.138123  0.525289  1.000000  0.832403  0.659512   \n",
       "3          3  0.712613  0.782345  0.190930  0.229841  0.560167  0.192150   \n",
       "4          2  0.197980  0.300174  0.130691  0.802940  1.000000  0.099832   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "78818      3 -0.012863  0.012005 -0.018340 -0.009440 -0.033045  0.098008   \n",
       "78819      2 -0.024249 -0.024640 -0.021435 -0.038170 -0.020313 -0.011039   \n",
       "78820      3 -0.059299  0.027528 -0.092111  0.653803  0.296419  1.000000   \n",
       "78821      3 -0.017755  0.003629 -0.001410  0.027432  0.071065  1.000000   \n",
       "78822      3 -0.018382  0.088634  0.170688  0.165125  1.000000  0.045117   \n",
       "\n",
       "              6         7         8  ...        90        91        92  \\\n",
       "0      1.000000  0.031178 -0.017721  ... -0.038098 -0.035813 -0.036718   \n",
       "1      0.099456  1.000000  0.614752  ... -0.042590 -0.045936 -0.045755   \n",
       "2     -0.047688  0.097587  0.014862  ... -0.019597 -0.046355 -0.016725   \n",
       "3      1.000000  0.783759 -0.025315  ...  0.063093 -0.020576 -0.054475   \n",
       "4     -0.009243  0.350689  0.281720  ... -0.063700 -0.047797 -0.032164   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "78818  1.000000  0.135341  0.145462  ... -0.069438 -0.023398 -0.072769   \n",
       "78819  1.000000  0.070916  0.121450  ... -0.035993 -0.035162 -0.035777   \n",
       "78820 -0.040911  0.927669 -0.082818  ... -0.184531 -0.013405  0.358282   \n",
       "78821  0.003929 -0.015067 -0.013738  ... -0.027309  0.003666 -0.051606   \n",
       "78822  0.030288  0.053111  0.020284  ... -0.068714  0.112066  0.031503   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "0     -0.038129 -0.033562 -0.038832 -0.038019 -0.037385 -0.034283 -0.039099  \n",
       "1     -0.045720 -0.045836 -0.045605 -0.044757 -0.046381 -0.043888 -0.045910  \n",
       "2     -0.110025 -0.074977 -0.020507 -0.081088 -0.117135  0.124611 -0.084323  \n",
       "3      0.056441 -0.033763 -0.020097 -0.038449 -0.030611  0.015814 -0.029294  \n",
       "4     -0.056727 -0.064532 -0.039657 -0.054612 -0.004255 -0.046586 -0.053346  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "78818 -0.036016 -0.059330 -0.054264  0.068717 -0.082765 -0.063361 -0.058121  \n",
       "78819 -0.035664 -0.036003 -0.035998 -0.035592 -0.035893 -0.035913 -0.035753  \n",
       "78820  0.185890  0.470470  0.242505  0.240674 -0.109372  0.071049  0.615857  \n",
       "78821 -0.022456  0.132588 -0.101000 -0.098870  0.060842 -0.020554 -0.055325  \n",
       "78822 -0.077510 -0.098591 -0.038509  0.203295  0.028393  0.095316 -0.097685  \n",
       "\n",
       "[78823 rows x 101 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.027216</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.053416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>-0.013044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029370</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>-0.023607</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>-0.031300</td>\n",
       "      <td>-0.031211</td>\n",
       "      <td>-0.030391</td>\n",
       "      <td>-0.030794</td>\n",
       "      <td>-0.031352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.717687</td>\n",
       "      <td>-0.036154</td>\n",
       "      <td>-0.047996</td>\n",
       "      <td>-0.023177</td>\n",
       "      <td>-0.026854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057607</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>-0.058113</td>\n",
       "      <td>-0.058398</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>-0.058231</td>\n",
       "      <td>-0.058531</td>\n",
       "      <td>-0.057697</td>\n",
       "      <td>-0.058402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.086143</td>\n",
       "      <td>-0.130700</td>\n",
       "      <td>-0.161727</td>\n",
       "      <td>-0.164976</td>\n",
       "      <td>0.577297</td>\n",
       "      <td>0.088806</td>\n",
       "      <td>-0.094891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172071</td>\n",
       "      <td>-0.170827</td>\n",
       "      <td>-0.171194</td>\n",
       "      <td>-0.168016</td>\n",
       "      <td>-0.170520</td>\n",
       "      <td>-0.171233</td>\n",
       "      <td>-0.169749</td>\n",
       "      <td>-0.171479</td>\n",
       "      <td>-0.171045</td>\n",
       "      <td>-0.170937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.017615</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>0.045736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>-0.006924</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.031255</td>\n",
       "      <td>-0.031199</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>-0.031537</td>\n",
       "      <td>-0.031499</td>\n",
       "      <td>-0.030825</td>\n",
       "      <td>-0.031589</td>\n",
       "      <td>-0.031559</td>\n",
       "      <td>-0.031543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.013705</td>\n",
       "      <td>-0.021804</td>\n",
       "      <td>-0.019416</td>\n",
       "      <td>-0.017692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022303</td>\n",
       "      <td>-0.021477</td>\n",
       "      <td>-0.016256</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015685</td>\n",
       "      <td>-0.024433</td>\n",
       "      <td>-0.024608</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>-0.025230</td>\n",
       "      <td>-0.025162</td>\n",
       "      <td>-0.025535</td>\n",
       "      <td>-0.024619</td>\n",
       "      <td>-0.024239</td>\n",
       "      <td>-0.024822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78818</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.196578</td>\n",
       "      <td>-0.190513</td>\n",
       "      <td>-0.189601</td>\n",
       "      <td>-0.190071</td>\n",
       "      <td>-0.134310</td>\n",
       "      <td>0.183397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165482</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192840</td>\n",
       "      <td>-0.195942</td>\n",
       "      <td>-0.191606</td>\n",
       "      <td>-0.193735</td>\n",
       "      <td>-0.195426</td>\n",
       "      <td>-0.195237</td>\n",
       "      <td>-0.193389</td>\n",
       "      <td>-0.194077</td>\n",
       "      <td>-0.193594</td>\n",
       "      <td>-0.195067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78819</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.061304</td>\n",
       "      <td>-0.057746</td>\n",
       "      <td>-0.045711</td>\n",
       "      <td>-0.055745</td>\n",
       "      <td>0.027025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027467</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>-0.034559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057418</td>\n",
       "      <td>-0.058316</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.059059</td>\n",
       "      <td>-0.059256</td>\n",
       "      <td>-0.057427</td>\n",
       "      <td>-0.058625</td>\n",
       "      <td>-0.056731</td>\n",
       "      <td>-0.053740</td>\n",
       "      <td>-0.050476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78820</th>\n",
       "      <td>3</td>\n",
       "      <td>0.919410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.540824</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>-0.046345</td>\n",
       "      <td>-0.049503</td>\n",
       "      <td>-0.072119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075835</td>\n",
       "      <td>-0.075814</td>\n",
       "      <td>-0.075791</td>\n",
       "      <td>-0.075812</td>\n",
       "      <td>-0.075693</td>\n",
       "      <td>-0.075819</td>\n",
       "      <td>-0.075829</td>\n",
       "      <td>-0.075817</td>\n",
       "      <td>-0.075885</td>\n",
       "      <td>-0.075873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78821</th>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135210</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.033463</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>-0.021910</td>\n",
       "      <td>0.156502</td>\n",
       "      <td>-0.017126</td>\n",
       "      <td>-0.022533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.032628</td>\n",
       "      <td>-0.032633</td>\n",
       "      <td>-0.032621</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>-0.032615</td>\n",
       "      <td>-0.032636</td>\n",
       "      <td>-0.032619</td>\n",
       "      <td>-0.032639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78822</th>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.469056</td>\n",
       "      <td>0.152544</td>\n",
       "      <td>0.104508</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.182801</td>\n",
       "      <td>0.184384</td>\n",
       "      <td>0.298208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099466</td>\n",
       "      <td>-0.099291</td>\n",
       "      <td>-0.097173</td>\n",
       "      <td>-0.096741</td>\n",
       "      <td>-0.098397</td>\n",
       "      <td>-0.098376</td>\n",
       "      <td>-0.099204</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>-0.098066</td>\n",
       "      <td>-0.097607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78823 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class         0         1         2         3         4         5  \\\n",
       "0          2 -0.030057 -0.027216 -0.016126  0.006233  0.053416  1.000000   \n",
       "1          3  0.124642  1.000000  0.608303  0.008150  0.717687 -0.036154   \n",
       "2          3 -0.086143 -0.130700 -0.161727 -0.164976  0.577297  0.088806   \n",
       "3          3 -0.017615  0.021958 -0.002278  0.045736  1.000000  0.026328   \n",
       "4          2 -0.013705 -0.021804 -0.019416 -0.017692  1.000000 -0.022303   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "78818      3 -0.196578 -0.190513 -0.189601 -0.190071 -0.134310  0.183397   \n",
       "78819      3 -0.061304 -0.057746 -0.045711 -0.055745  0.027025  1.000000   \n",
       "78820      3  0.919410  1.000000  0.368404  0.540824  0.348019  0.020499   \n",
       "78821      3  1.000000  0.135210  0.001155  0.033463  0.043519 -0.021910   \n",
       "78822      3  1.000000  0.408754  0.469056  0.152544  0.104508  0.074612   \n",
       "\n",
       "              6         7         8  ...        40        41        42  \\\n",
       "0      0.031091  0.015465 -0.013044  ... -0.029370 -0.030332 -0.023607   \n",
       "1     -0.047996 -0.023177 -0.026854  ... -0.057607 -0.057537 -0.058274   \n",
       "2     -0.094891  1.000000  0.151082  ... -0.172071 -0.170827 -0.171194   \n",
       "3     -0.006924 -0.015067  0.009032  ... -0.031494 -0.031255 -0.031199   \n",
       "4     -0.021477 -0.016256 -0.003198  ... -0.015685 -0.024433 -0.024608   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "78818  1.000000  0.165482  0.058812  ... -0.192840 -0.195942 -0.191606   \n",
       "78819  0.027467 -0.001114 -0.034559  ... -0.057418 -0.058316 -0.053983   \n",
       "78820 -0.046345 -0.049503 -0.072119  ... -0.075835 -0.075814 -0.075791   \n",
       "78821  0.156502 -0.017126 -0.022533  ... -0.032626 -0.032628 -0.032633   \n",
       "78822  0.182801  0.184384  0.298208  ... -0.099466 -0.099291 -0.097173   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "0     -0.031657 -0.028310 -0.031300 -0.031211 -0.030391 -0.030794 -0.031352  \n",
       "1     -0.058113 -0.058398 -0.058491 -0.058231 -0.058531 -0.057697 -0.058402  \n",
       "2     -0.168016 -0.170520 -0.171233 -0.169749 -0.171479 -0.171045 -0.170937  \n",
       "3     -0.030970 -0.031537 -0.031499 -0.030825 -0.031589 -0.031559 -0.031543  \n",
       "4     -0.025368 -0.025230 -0.025162 -0.025535 -0.024619 -0.024239 -0.024822  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "78818 -0.193735 -0.195426 -0.195237 -0.193389 -0.194077 -0.193594 -0.195067  \n",
       "78819 -0.059059 -0.059256 -0.057427 -0.058625 -0.056731 -0.053740 -0.050476  \n",
       "78820 -0.075812 -0.075693 -0.075819 -0.075829 -0.075817 -0.075885 -0.075873  \n",
       "78821 -0.032621 -0.032624 -0.032624 -0.032615 -0.032636 -0.032619 -0.032639  \n",
       "78822 -0.096741 -0.098397 -0.098376 -0.099204 -0.097753 -0.098066 -0.097607  \n",
       "\n",
       "[78823 rows x 51 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.311070</td>\n",
       "      <td>0.602284</td>\n",
       "      <td>-0.295414</td>\n",
       "      <td>-0.135353</td>\n",
       "      <td>-0.272519</td>\n",
       "      <td>0.470501</td>\n",
       "      <td>0.055555</td>\n",
       "      <td>0.350612</td>\n",
       "      <td>0.283464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121085</td>\n",
       "      <td>0.458755</td>\n",
       "      <td>-0.287152</td>\n",
       "      <td>0.004847</td>\n",
       "      <td>-0.280585</td>\n",
       "      <td>-0.125173</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>-0.288257</td>\n",
       "      <td>0.115472</td>\n",
       "      <td>0.013447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.036019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044621</td>\n",
       "      <td>-0.017574</td>\n",
       "      <td>0.095875</td>\n",
       "      <td>-0.015582</td>\n",
       "      <td>-0.010842</td>\n",
       "      <td>-0.030141</td>\n",
       "      <td>-0.020132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029158</td>\n",
       "      <td>-0.034659</td>\n",
       "      <td>-0.034904</td>\n",
       "      <td>-0.035061</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>-0.035320</td>\n",
       "      <td>-0.032870</td>\n",
       "      <td>-0.015724</td>\n",
       "      <td>-0.032703</td>\n",
       "      <td>-0.035713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.102323</td>\n",
       "      <td>0.267455</td>\n",
       "      <td>0.789374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.097091</td>\n",
       "      <td>-0.016171</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.598286</td>\n",
       "      <td>-0.049404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096673</td>\n",
       "      <td>-0.099150</td>\n",
       "      <td>-0.100942</td>\n",
       "      <td>-0.101388</td>\n",
       "      <td>-0.100168</td>\n",
       "      <td>-0.101975</td>\n",
       "      <td>-0.100745</td>\n",
       "      <td>-0.101671</td>\n",
       "      <td>-0.101034</td>\n",
       "      <td>-0.101691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.560520</td>\n",
       "      <td>-0.450013</td>\n",
       "      <td>0.732302</td>\n",
       "      <td>0.579498</td>\n",
       "      <td>0.405977</td>\n",
       "      <td>-0.554563</td>\n",
       "      <td>0.420540</td>\n",
       "      <td>0.075622</td>\n",
       "      <td>-0.501731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160053</td>\n",
       "      <td>0.183239</td>\n",
       "      <td>0.330863</td>\n",
       "      <td>-0.430148</td>\n",
       "      <td>-0.050102</td>\n",
       "      <td>-0.469018</td>\n",
       "      <td>-0.378305</td>\n",
       "      <td>-0.347293</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>0.100423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.062265</td>\n",
       "      <td>-0.059779</td>\n",
       "      <td>0.269739</td>\n",
       "      <td>0.126958</td>\n",
       "      <td>0.393020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914650</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>-0.032771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061744</td>\n",
       "      <td>-0.062252</td>\n",
       "      <td>-0.062077</td>\n",
       "      <td>-0.062227</td>\n",
       "      <td>-0.062246</td>\n",
       "      <td>-0.062236</td>\n",
       "      <td>-0.062251</td>\n",
       "      <td>-0.062113</td>\n",
       "      <td>-0.062113</td>\n",
       "      <td>-0.062244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78818</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.269766</td>\n",
       "      <td>0.196617</td>\n",
       "      <td>0.072544</td>\n",
       "      <td>-0.194465</td>\n",
       "      <td>-0.109302</td>\n",
       "      <td>0.390684</td>\n",
       "      <td>-0.089140</td>\n",
       "      <td>-0.155741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>0.595054</td>\n",
       "      <td>0.099036</td>\n",
       "      <td>0.045020</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>-0.061369</td>\n",
       "      <td>-0.189697</td>\n",
       "      <td>0.172433</td>\n",
       "      <td>-0.179482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78819</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.027526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006795</td>\n",
       "      <td>-0.026603</td>\n",
       "      <td>-0.024834</td>\n",
       "      <td>-0.026691</td>\n",
       "      <td>-0.027017</td>\n",
       "      <td>-0.009539</td>\n",
       "      <td>-0.011659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>-0.025038</td>\n",
       "      <td>-0.027022</td>\n",
       "      <td>-0.024138</td>\n",
       "      <td>-0.027477</td>\n",
       "      <td>-0.025119</td>\n",
       "      <td>-0.027264</td>\n",
       "      <td>-0.026643</td>\n",
       "      <td>-0.022561</td>\n",
       "      <td>-0.025719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78820</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.119000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021042</td>\n",
       "      <td>0.098366</td>\n",
       "      <td>-0.013572</td>\n",
       "      <td>-0.012918</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.045739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008321</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>-0.042712</td>\n",
       "      <td>-0.104822</td>\n",
       "      <td>0.204603</td>\n",
       "      <td>0.005783</td>\n",
       "      <td>-0.094751</td>\n",
       "      <td>-0.004194</td>\n",
       "      <td>-0.043655</td>\n",
       "      <td>-0.109560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78821</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.065489</td>\n",
       "      <td>-0.026753</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.353314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-0.028185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061065</td>\n",
       "      <td>-0.064014</td>\n",
       "      <td>-0.063624</td>\n",
       "      <td>-0.065410</td>\n",
       "      <td>-0.065471</td>\n",
       "      <td>-0.064557</td>\n",
       "      <td>-0.064141</td>\n",
       "      <td>-0.065216</td>\n",
       "      <td>-0.062876</td>\n",
       "      <td>-0.065382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78822</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.151503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.087561</td>\n",
       "      <td>0.028637</td>\n",
       "      <td>0.187530</td>\n",
       "      <td>0.135972</td>\n",
       "      <td>-0.146437</td>\n",
       "      <td>0.414554</td>\n",
       "      <td>-0.060040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155516</td>\n",
       "      <td>-0.143649</td>\n",
       "      <td>0.203902</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>0.087228</td>\n",
       "      <td>-0.100472</td>\n",
       "      <td>-0.122130</td>\n",
       "      <td>-0.073658</td>\n",
       "      <td>-0.097420</td>\n",
       "      <td>-0.045243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78823 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class         0         1         2         3         4         5  \\\n",
       "0          3 -0.311070  0.602284 -0.295414 -0.135353 -0.272519  0.470501   \n",
       "1          1 -0.036019  1.000000  0.044621 -0.017574  0.095875 -0.015582   \n",
       "2          1 -0.102323  0.267455  0.789374  1.000000 -0.097091 -0.016171   \n",
       "3          3 -0.560520 -0.450013  0.732302  0.579498  0.405977 -0.554563   \n",
       "4          1 -0.062265 -0.059779  0.269739  0.126958  0.393020  1.000000   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "78818      3 -0.269766  0.196617  0.072544 -0.194465 -0.109302  0.390684   \n",
       "78819      3 -0.027526  1.000000 -0.006795 -0.026603 -0.024834 -0.026691   \n",
       "78820      3 -0.119000  1.000000 -0.021042  0.098366 -0.013572 -0.012918   \n",
       "78821      1 -0.065489 -0.026753  0.904410  0.025899  0.353314  1.000000   \n",
       "78822      3 -0.151503  1.000000 -0.087561  0.028637  0.187530  0.135972   \n",
       "\n",
       "              6         7         8  ...        40        41        42  \\\n",
       "0      0.055555  0.350612  0.283464  ... -0.121085  0.458755 -0.287152   \n",
       "1     -0.010842 -0.030141 -0.020132  ... -0.029158 -0.034659 -0.034904   \n",
       "2      0.186275  0.598286 -0.049404  ... -0.096673 -0.099150 -0.100942   \n",
       "3      0.420540  0.075622 -0.501731  ...  0.160053  0.183239  0.330863   \n",
       "4      0.914650 -0.019336 -0.032771  ... -0.061744 -0.062252 -0.062077   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "78818 -0.089140 -0.155741  1.000000  ...  0.053338  0.100374  0.595054   \n",
       "78819 -0.027017 -0.009539 -0.011659  ... -0.027411 -0.025038 -0.027022   \n",
       "78820  0.008290  0.012824  0.045739  ... -0.008321  0.119679 -0.042712   \n",
       "78821  0.005112 -0.002746 -0.028185  ... -0.061065 -0.064014 -0.063624   \n",
       "78822 -0.146437  0.414554 -0.060040  ...  0.155516 -0.143649  0.203902   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "0      0.004847 -0.280585 -0.125173  0.010386 -0.288257  0.115472  0.013447  \n",
       "1     -0.035061  0.006946 -0.035320 -0.032870 -0.015724 -0.032703 -0.035713  \n",
       "2     -0.101388 -0.100168 -0.101975 -0.100745 -0.101671 -0.101034 -0.101691  \n",
       "3     -0.430148 -0.050102 -0.469018 -0.378305 -0.347293 -0.307555  0.100423  \n",
       "4     -0.062227 -0.062246 -0.062236 -0.062251 -0.062113 -0.062113 -0.062244  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "78818  0.099036  0.045020  0.042506 -0.061369 -0.189697  0.172433 -0.179482  \n",
       "78819 -0.024138 -0.027477 -0.025119 -0.027264 -0.026643 -0.022561 -0.025719  \n",
       "78820 -0.104822  0.204603  0.005783 -0.094751 -0.004194 -0.043655 -0.109560  \n",
       "78821 -0.065410 -0.065471 -0.064557 -0.064141 -0.065216 -0.062876 -0.065382  \n",
       "78822 -0.002047  0.087228 -0.100472 -0.122130 -0.073658 -0.097420 -0.045243  \n",
       "\n",
       "[78823 rows x 51 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['class'] = data1['class'].apply(lambda x: 0 if x == 1 else (1 if x == 2 else 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.027216</td>\n",
       "      <td>-0.016126</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.053416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>0.015465</td>\n",
       "      <td>-0.013044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029370</td>\n",
       "      <td>-0.030332</td>\n",
       "      <td>-0.023607</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>-0.031300</td>\n",
       "      <td>-0.031211</td>\n",
       "      <td>-0.030391</td>\n",
       "      <td>-0.030794</td>\n",
       "      <td>-0.031352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.124642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.717687</td>\n",
       "      <td>-0.036154</td>\n",
       "      <td>-0.047996</td>\n",
       "      <td>-0.023177</td>\n",
       "      <td>-0.026854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057607</td>\n",
       "      <td>-0.057537</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>-0.058113</td>\n",
       "      <td>-0.058398</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>-0.058231</td>\n",
       "      <td>-0.058531</td>\n",
       "      <td>-0.057697</td>\n",
       "      <td>-0.058402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.086143</td>\n",
       "      <td>-0.130700</td>\n",
       "      <td>-0.161727</td>\n",
       "      <td>-0.164976</td>\n",
       "      <td>0.577297</td>\n",
       "      <td>0.088806</td>\n",
       "      <td>-0.094891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172071</td>\n",
       "      <td>-0.170827</td>\n",
       "      <td>-0.171194</td>\n",
       "      <td>-0.168016</td>\n",
       "      <td>-0.170520</td>\n",
       "      <td>-0.171233</td>\n",
       "      <td>-0.169749</td>\n",
       "      <td>-0.171479</td>\n",
       "      <td>-0.171045</td>\n",
       "      <td>-0.170937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.017615</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>0.045736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>-0.006924</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031494</td>\n",
       "      <td>-0.031255</td>\n",
       "      <td>-0.031199</td>\n",
       "      <td>-0.030970</td>\n",
       "      <td>-0.031537</td>\n",
       "      <td>-0.031499</td>\n",
       "      <td>-0.030825</td>\n",
       "      <td>-0.031589</td>\n",
       "      <td>-0.031559</td>\n",
       "      <td>-0.031543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.013705</td>\n",
       "      <td>-0.021804</td>\n",
       "      <td>-0.019416</td>\n",
       "      <td>-0.017692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022303</td>\n",
       "      <td>-0.021477</td>\n",
       "      <td>-0.016256</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015685</td>\n",
       "      <td>-0.024433</td>\n",
       "      <td>-0.024608</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>-0.025230</td>\n",
       "      <td>-0.025162</td>\n",
       "      <td>-0.025535</td>\n",
       "      <td>-0.024619</td>\n",
       "      <td>-0.024239</td>\n",
       "      <td>-0.024822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78818</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.196578</td>\n",
       "      <td>-0.190513</td>\n",
       "      <td>-0.189601</td>\n",
       "      <td>-0.190071</td>\n",
       "      <td>-0.134310</td>\n",
       "      <td>0.183397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165482</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192840</td>\n",
       "      <td>-0.195942</td>\n",
       "      <td>-0.191606</td>\n",
       "      <td>-0.193735</td>\n",
       "      <td>-0.195426</td>\n",
       "      <td>-0.195237</td>\n",
       "      <td>-0.193389</td>\n",
       "      <td>-0.194077</td>\n",
       "      <td>-0.193594</td>\n",
       "      <td>-0.195067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78819</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.061304</td>\n",
       "      <td>-0.057746</td>\n",
       "      <td>-0.045711</td>\n",
       "      <td>-0.055745</td>\n",
       "      <td>0.027025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027467</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>-0.034559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057418</td>\n",
       "      <td>-0.058316</td>\n",
       "      <td>-0.053983</td>\n",
       "      <td>-0.059059</td>\n",
       "      <td>-0.059256</td>\n",
       "      <td>-0.057427</td>\n",
       "      <td>-0.058625</td>\n",
       "      <td>-0.056731</td>\n",
       "      <td>-0.053740</td>\n",
       "      <td>-0.050476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78820</th>\n",
       "      <td>2</td>\n",
       "      <td>0.919410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.540824</td>\n",
       "      <td>0.348019</td>\n",
       "      <td>0.020499</td>\n",
       "      <td>-0.046345</td>\n",
       "      <td>-0.049503</td>\n",
       "      <td>-0.072119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075835</td>\n",
       "      <td>-0.075814</td>\n",
       "      <td>-0.075791</td>\n",
       "      <td>-0.075812</td>\n",
       "      <td>-0.075693</td>\n",
       "      <td>-0.075819</td>\n",
       "      <td>-0.075829</td>\n",
       "      <td>-0.075817</td>\n",
       "      <td>-0.075885</td>\n",
       "      <td>-0.075873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78821</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135210</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.033463</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>-0.021910</td>\n",
       "      <td>0.156502</td>\n",
       "      <td>-0.017126</td>\n",
       "      <td>-0.022533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.032628</td>\n",
       "      <td>-0.032633</td>\n",
       "      <td>-0.032621</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>-0.032624</td>\n",
       "      <td>-0.032615</td>\n",
       "      <td>-0.032636</td>\n",
       "      <td>-0.032619</td>\n",
       "      <td>-0.032639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78822</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.469056</td>\n",
       "      <td>0.152544</td>\n",
       "      <td>0.104508</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.182801</td>\n",
       "      <td>0.184384</td>\n",
       "      <td>0.298208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099466</td>\n",
       "      <td>-0.099291</td>\n",
       "      <td>-0.097173</td>\n",
       "      <td>-0.096741</td>\n",
       "      <td>-0.098397</td>\n",
       "      <td>-0.098376</td>\n",
       "      <td>-0.099204</td>\n",
       "      <td>-0.097753</td>\n",
       "      <td>-0.098066</td>\n",
       "      <td>-0.097607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78823 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class         0         1         2         3         4         5  \\\n",
       "0          1 -0.030057 -0.027216 -0.016126  0.006233  0.053416  1.000000   \n",
       "1          2  0.124642  1.000000  0.608303  0.008150  0.717687 -0.036154   \n",
       "2          2 -0.086143 -0.130700 -0.161727 -0.164976  0.577297  0.088806   \n",
       "3          2 -0.017615  0.021958 -0.002278  0.045736  1.000000  0.026328   \n",
       "4          1 -0.013705 -0.021804 -0.019416 -0.017692  1.000000 -0.022303   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "78818      2 -0.196578 -0.190513 -0.189601 -0.190071 -0.134310  0.183397   \n",
       "78819      2 -0.061304 -0.057746 -0.045711 -0.055745  0.027025  1.000000   \n",
       "78820      2  0.919410  1.000000  0.368404  0.540824  0.348019  0.020499   \n",
       "78821      2  1.000000  0.135210  0.001155  0.033463  0.043519 -0.021910   \n",
       "78822      2  1.000000  0.408754  0.469056  0.152544  0.104508  0.074612   \n",
       "\n",
       "              6         7         8  ...        40        41        42  \\\n",
       "0      0.031091  0.015465 -0.013044  ... -0.029370 -0.030332 -0.023607   \n",
       "1     -0.047996 -0.023177 -0.026854  ... -0.057607 -0.057537 -0.058274   \n",
       "2     -0.094891  1.000000  0.151082  ... -0.172071 -0.170827 -0.171194   \n",
       "3     -0.006924 -0.015067  0.009032  ... -0.031494 -0.031255 -0.031199   \n",
       "4     -0.021477 -0.016256 -0.003198  ... -0.015685 -0.024433 -0.024608   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "78818  1.000000  0.165482  0.058812  ... -0.192840 -0.195942 -0.191606   \n",
       "78819  0.027467 -0.001114 -0.034559  ... -0.057418 -0.058316 -0.053983   \n",
       "78820 -0.046345 -0.049503 -0.072119  ... -0.075835 -0.075814 -0.075791   \n",
       "78821  0.156502 -0.017126 -0.022533  ... -0.032626 -0.032628 -0.032633   \n",
       "78822  0.182801  0.184384  0.298208  ... -0.099466 -0.099291 -0.097173   \n",
       "\n",
       "             43        44        45        46        47        48        49  \n",
       "0     -0.031657 -0.028310 -0.031300 -0.031211 -0.030391 -0.030794 -0.031352  \n",
       "1     -0.058113 -0.058398 -0.058491 -0.058231 -0.058531 -0.057697 -0.058402  \n",
       "2     -0.168016 -0.170520 -0.171233 -0.169749 -0.171479 -0.171045 -0.170937  \n",
       "3     -0.030970 -0.031537 -0.031499 -0.030825 -0.031589 -0.031559 -0.031543  \n",
       "4     -0.025368 -0.025230 -0.025162 -0.025535 -0.024619 -0.024239 -0.024822  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "78818 -0.193735 -0.195426 -0.195237 -0.193389 -0.194077 -0.193594 -0.195067  \n",
       "78819 -0.059059 -0.059256 -0.057427 -0.058625 -0.056731 -0.053740 -0.050476  \n",
       "78820 -0.075812 -0.075693 -0.075819 -0.075829 -0.075817 -0.075885 -0.075873  \n",
       "78821 -0.032621 -0.032624 -0.032624 -0.032615 -0.032636 -0.032619 -0.032639  \n",
       "78822 -0.096741 -0.098397 -0.098376 -0.099204 -0.097753 -0.098066 -0.097607  \n",
       "\n",
       "[78823 rows x 51 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1[[str(x) for x in range(50)]]\n",
    "y = data1['class']\n",
    "X = X.values\n",
    "y = y.values\n",
    "# y = torch.tensor(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=34)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ms.fit_transform(X_train)\n",
    "X_test = ms.fit_transform(X_test)\n",
    "# y_train = y_train.reshape(-1, 1)\n",
    "# y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97204102,  0.05565637, -0.03050711, ..., -0.10396622,\n",
       "        -0.10708707, -0.11608528],\n",
       "       [-0.06783397, -0.06778439, -0.06220271, ..., -0.06615854,\n",
       "        -0.06337908, -0.00742759],\n",
       "       [-0.02546111, -0.02505541, -0.0245985 , ..., -0.02555124,\n",
       "        -0.024686  , -0.02069945],\n",
       "       ...,\n",
       "       [-0.06632779,  0.05824382,  0.08819027, ..., -0.07748754,\n",
       "        -0.07818329, -0.07673419],\n",
       "       [-0.03275923, -0.02356955, -0.0218401 , ..., -0.03407454,\n",
       "        -0.02735483, -0.03268085],\n",
       "       [-0.02716259,  0.0521438 ,  0.0069933 , ..., -0.03735497,\n",
       "        -0.03803071, -0.0374999 ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = customDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "test_data = customDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 64, shuffle= True)\n",
    "test_loader = DataLoader(test_data, batch_size = 64, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1000 [00:33<18:07,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22529/4135733335.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 32)\n",
    "        self.fc2 = nn.Linear(32,32)\n",
    "        self.fc3 = nn.Linear(32,3)\n",
    "        self.droput = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.view(-1,50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.droput(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CustomModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    cost = 0.0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     classes = {0: \"acute triangle\", 1: \"right triangle\", 2: \"obtuse triangle\"}\n",
    "#     inputs = torch.FloatTensor([\n",
    "#         [9.02, 9.77, 9.96], # 0 | acute triangle\n",
    "#         [8.01, 8.08, 8.32], # 0 | acute triangle\n",
    "#         [3.55, 5.15, 6.26], # 1 | right triangle\n",
    "#         [3.32, 3.93, 5.14], # 1 | right triangle\n",
    "#         [4.39, 5.56, 9.99], # 2 | obtuse triangle\n",
    "#         [3.01, 3.08, 9.98], # 2 | obtuse triangle\n",
    "#         [5.21, 5.38, 5.39], # 0 | acute triangle\n",
    "#         [3.85, 6.23, 7.32], # 1 | right triangle\n",
    "#         [4.16, 4.98, 8.54], # 2 | obtuse triangle\n",
    "#     ]).to(device)\n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "#     print('---------')\n",
    "#     print(outputs)\n",
    "#     print(torch.round(F.softmax(outputs, dim=1), decimals=2))\n",
    "#     print(outputs.argmax(1))\n",
    "#     print(list(map(classes.get, outputs.argmax(1).tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 8\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = int(input_dim/nodes)\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(MultiClassClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52811"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Multiclass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(4, 8)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(neuNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(50, 32)\n",
    "        self.fc2 = nn.Linear(32,32)\n",
    "        self.fc3 = nn.Linear(32,3)\n",
    "        self.droput = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.view(-1,50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.droput(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.droput(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neuNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuNet(\n",
       "  (fc1): Linear(in_features=50, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
       "  (droput): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "MLP_criterion = nn.CrossEntropyLoss()\n",
    "Model_optimizer = torch.optim.SGD(model.parameters(),lr =learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    # print(y_pred_tag)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    # print(correct_results_sum)\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12000/780857326.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0miteration_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "print_epoch = 5\n",
    "for epoch in range(epochs):\n",
    "    iteration_loss = 0.\n",
    "    iteration_accuracy = 0.\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss = MLP_criterion(y_pred, y.reshape(-1, 1).float())\n",
    "        \n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred, y)\n",
    "        Model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Model_optimizer.step()\n",
    "    \n",
    "    if epoch % print_epoch == 0:\n",
    "        print(f'Train: epoch: {epoch} - loss: {(iteration_loss/(i+1)):.5f}; acc: {(iteration_accuracy/(i+1)):.3f}')\n",
    "    \n",
    "    iteration_loss = 0.\n",
    "    iteration_accuracy = 0.\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X.float())\n",
    "        loss = MLP_criterion(y_pred, y.reshape(-1, 1).float())\n",
    "        \n",
    "        iteration_loss += loss\n",
    "        iteration_accuracy += accuracy(y_pred, y)\n",
    "        Model_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        Model_optimizer.step()\n",
    "    \n",
    "    if epoch % print_epoch == 0:\n",
    "        print(f'Test: epoch: {epoch} - loss: {(iteration_loss/(i+1)):.5f}; acc: {(iteration_accuracy/(i+1)):.3f}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Cost : 0.205\n",
      "Epoch : 2000, Cost : 0.131\n",
      "Epoch : 3000, Cost : 0.096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12000/4218192311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvFL/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12000/4218192311.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.a = df.iloc[:, 0].values\n",
    "        self.b = df.iloc[:, 1].values\n",
    "        self.c = df.iloc[:, 2].values\n",
    "        self.y = df.iloc[:, 3].values\n",
    "        self.y = list(map(self.string_to_vector, self.y))\n",
    "        self.length = len(df)\n",
    "\n",
    "    def string_to_vector(self, value):\n",
    "        data = {\"acute triangle\": 0, \"right triangle\": 1, \"obtuse triangle\": 2}\n",
    "        return data.get(value, None)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor(sorted([self.a[index], self.b[index], self.c[index]]))\n",
    "        y = torch.LongTensor(self.y)[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(3, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(\"../Data/dataset.csv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CustomModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    cost = 0.0\n",
    "\n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss\n",
    "\n",
    "    cost = cost / len(train_dataloader)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    classes = {0: \"acute triangle\", 1: \"right triangle\", 2: \"obtuse triangle\"}\n",
    "    inputs = torch.FloatTensor([\n",
    "        [9.02, 9.77, 9.96], # 0 | acute triangle\n",
    "        [8.01, 8.08, 8.32], # 0 | acute triangle\n",
    "        [3.55, 5.15, 6.26], # 1 | right triangle\n",
    "        [3.32, 3.93, 5.14], # 1 | right triangle\n",
    "        [4.39, 5.56, 9.99], # 2 | obtuse triangle\n",
    "        [3.01, 3.08, 9.98], # 2 | obtuse triangle\n",
    "        [5.21, 5.38, 5.39], # 0 | acute triangle\n",
    "        [3.85, 6.23, 7.32], # 1 | right triangle\n",
    "        [4.16, 4.98, 8.54], # 2 | obtuse triangle\n",
    "    ]).to(device)\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    print('---------')\n",
    "    print(outputs)\n",
    "    print(torch.round(F.softmax(outputs, dim=1), decimals=2))\n",
    "    print(outputs.argmax(1))\n",
    "    print(list(map(classes.get, outputs.argmax(1).tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhmoon/venvFL/env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        self.a = df.iloc[:, 0].values\n",
    "        self.b = df.iloc[:, 1].values\n",
    "        self.c = df.iloc[:, 2].values\n",
    "        self.y = df.iloc[:, 3].values\n",
    "        self.y = list(map(self.string_to_vector, self.y))\n",
    "        self.length = len(df)\n",
    "\n",
    "    def string_to_vector(self, value):\n",
    "        data = {\"acute triangle\": 0, \"right triangle\": 1, \"obtuse triangle\": 2}\n",
    "        return data.get(value, None)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor(sorted([self.a[index], self.b[index], self.c[index]]))\n",
    "        y = torch.LongTensor(self.y)[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\"../Data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.16, 9.46, 7.23, ..., 9.24, 7.86, 6.11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 0].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
